{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark\n",
    "\n",
    "## 指路\n",
    "大数据方面, 由于我是野路子出家, 对于 pandas 等库不熟悉, 对大数据运作思维也不清晰, 犯了不少错.\n",
    "希望能通过这个总结, 能帮助自己或其他人, 减少误区, 加深理解.\n",
    "\n",
    "pyspark 的核心是 dataframe, dataframe 即数据帧, 而数据的核心无非三点\n",
    "1. 创建/导入\n",
    "2. 处理\n",
    "3. 导出\n",
    "\n",
    "---\n",
    "创建dataframe的方式有\n",
    "1. 通过 `spark.createDataFrame()`, 直接使用变量数据创建 dataframe.\n",
    "2. 通过 `spark.read.xxx()`, 从文件/rdd中读取数据. 可以读取 csv/json/rdd 等多种格式的文件.\n",
    "\n",
    "所以, 当你需要导入数据, 创建 dataframe 时, 无外乎如上两种方式. 根据你的需求, 在合适的范围内查找API.\n",
    "\n",
    "---\n",
    "dataframe 处理方式有\n",
    "1. 使用 pyspark api, 如 `df.filter()` 等.\n",
    "2. 使用sql思维, 将 df 注册为表, 然后写 sql.\n",
    "\n",
    "在 spark2 中, 使用 sql 思维处理数据较为普遍. 多数情况下, 我们通过 `createOrReplaceTempView` 将 dataframe \n",
    "注册为临时表, 然后使用 `spark.sql(sql)` 执行sql, 处理数据.\n",
    "\n",
    "或者, 我们转变思维, 使用dataframe的api处理数据, 如 `filter, groupby, alias` 等.\n",
    "\n",
    "根据我的经验, 我发现无论是 pyspark, 还是 pandas, 其数据处理的很多思维都与写sql类似, 思考问题是可以从这个方面考虑, \n",
    "使用sql想出逻辑后, 如不适合使用sql处理时, 转换为相应的api.\n",
    "\n",
    "ps, 处理逻辑与 sql 类似的原因, 我猜测是因为\n",
    "1. SQL 全名 结构化查询语言, 本身就是为了数据处理/查询而开发的.\n",
    "2. dataframe 的结构与数据库类似, 都是行列式, 其处理都是以行为单位, 逐行处理.\n",
    "    其思维也因此有类似之处.\n",
    "\n",
    "---\n",
    "dataframe 输出的方式有\n",
    "1. 通过 `df.collect()` 等方式将数据导出到变量中\n",
    "2. 通过 `df.write.format(format).mode(SaveMode.xxx).xxx`, 可以将数据导入到文件(csv/json等)中, \n",
    "    也可以直接导入到 hive/mysql 表中.\n",
    "\n",
    "所以, 当你需要导出 dataframe 中的数据时, 无外乎如上两种方式. 根据你的需求, 在合适的范围内查找API.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
